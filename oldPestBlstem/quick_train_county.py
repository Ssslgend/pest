# quick_train_county.py
"""
å¿«é€Ÿå¿çº§BiLSTMè®­ç»ƒè„šæœ¬
ç®€åŒ–ç‰ˆæœ¬ï¼Œç”¨äºå¿«é€Ÿæµ‹è¯•å’ŒéªŒè¯
"""

import torch
import torch.nn as nn
from torch.optim import Adam
import pandas as pd
import numpy as np
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix, roc_auc_score
import joblib
import os
from datetime import datetime

# æ£€æŸ¥æ˜¯å¦æœ‰GPU
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ä½¿ç”¨è®¾å¤‡: {device}")

# å¯¼å…¥BiLSTMæ¨¡å‹
from model.bilstm import BiLSTMModel

def load_and_prepare_data(data_path, sequence_length=8):
    """
    åŠ è½½å’Œå‡†å¤‡æ•°æ® - äºŒåˆ†ç±»ç‰ˆæœ¬ï¼ˆæ˜¯å¦æœ‰ç—…è™«å®³ï¼‰
    """
    print(f"åŠ è½½æ•°æ®: {data_path}")
    data = pd.read_csv(data_path)
    
    # ç‰¹å¾åˆ—
    feature_columns = [
        'Temperature', 'Humidity', 'Rainfall', 'WS', 'WD', 'Pressure', 
        'Sunshine', 'Visibility', 'Temperature_MA', 'Humidity_MA', 
        'Rainfall_MA', 'Pressure_MA', 'Temp_7day_MA', 'Humidity_7day_MA', 
        'Rainfall_7day_MA', 'Temp_Change', 'Cumulative_Rainfall_7day', 
        'Temp_Humidity_Index'
    ]
    
    # æŒ‰å¿å’Œæ—¶é—´æ’åº
    data = data.sort_values(['county_name', 'year', 'month', 'day'])
    
    # æå–ç‰¹å¾å’Œæ ‡ç­¾
    features = data[feature_columns].values
    
    # äºŒåˆ†ç±»è½¬æ¢ï¼šValue_Class > 0 è¡¨ç¤ºæœ‰ç—…è™«å®³ï¼ŒValue_Class == 0 è¡¨ç¤ºæ— ç—…è™«å®³
    original_labels = data['Value_Class'].values
    labels = (original_labels > 0).astype(int)  # 0=æ— ç—…è™«å®³, 1=æœ‰ç—…è™«å®³
    
    print(f"åŸå§‹æ ‡ç­¾åˆ†å¸ƒ:")
    unique_orig, counts_orig = np.unique(original_labels, return_counts=True)
    for u, c in zip(unique_orig, counts_orig):
        print(f"  åŸå§‹ç±»åˆ« {u}: {c} æ ·æœ¬")
    
    print(f"äºŒåˆ†ç±»æ ‡ç­¾åˆ†å¸ƒ:")
    unique, counts = np.unique(labels, return_counts=True)
    for u, c in zip(unique, counts):
        status = "æœ‰ç—…è™«å®³" if u == 1 else "æ— ç—…è™«å®³"
        print(f"  {status} (ç±»åˆ« {u}): {c} æ ·æœ¬")
    
    # æ ‡å‡†åŒ–
    scaler = joblib.load('datas/shandong_pest_data/spatial_meteorological_scaler.joblib')
    features = scaler.transform(features)
    
    print(f"ç‰¹å¾å½¢çŠ¶: {features.shape}")
    print(f"æ ‡ç­¾å½¢çŠ¶: {labels.shape}")
    
    # åˆ›å»ºåºåˆ—
    sequences = []
    sequence_labels = []
    
    counties = data['county_name'].unique()
    print(f"å¤„ç† {len(counties)} ä¸ªå¿çš„æ•°æ®...")
    
    # ä½¿ç”¨æ‰€æœ‰å¿çš„æ•°æ®
    for county in counties:
        county_data = data[data['county_name'] == county]
        county_indices = county_data.index
        
        county_mask = data.index.isin(county_indices)
        county_features = features[county_mask]
        county_labels = labels[county_mask]
        
        # åˆ›å»ºåºåˆ—
        for i in range(len(county_features) - sequence_length + 1):
            sequences.append(county_features[i:i + sequence_length])
            sequence_labels.append(county_labels[i + sequence_length - 1])
    
    sequences = np.array(sequences)
    sequence_labels = np.array(sequence_labels)
    
    print(f"åºåˆ—å½¢çŠ¶: {sequences.shape}")
    print(f"åºåˆ—æ ‡ç­¾å½¢çŠ¶: {sequence_labels.shape}")
    
    # ç»Ÿè®¡æœ€ç»ˆæ ‡ç­¾åˆ†å¸ƒ
    unique, counts = np.unique(sequence_labels, return_counts=True)
    print("æœ€ç»ˆåºåˆ—æ ‡ç­¾åˆ†å¸ƒ:")
    for u, c in zip(unique, counts):
        status = "æœ‰ç—…è™«å®³" if u == 1 else "æ— ç—…è™«å®³"
        print(f"  {status} (ç±»åˆ« {u}): {c} æ ·æœ¬ ({c/len(sequence_labels)*100:.1f}%)")
    
    return sequences, sequence_labels, len(feature_columns)

def train_model():
    """è®­ç»ƒæ¨¡å‹ - åŸºäºåŸå§‹æ¡†æ¶"""
    # åŠ è½½æ•°æ®
    sequences, labels, input_size = load_and_prepare_data('datas/shandong_pest_data/spatial_train_data.csv', 8)
    
    # è½¬æ¢ä¸ºPyTorchå¼ é‡
    X = torch.FloatTensor(sequences).to(device)
    y = torch.LongTensor(labels).to(device)
    
    # åˆ›å»ºæ¨¡å‹é…ç½® - äºŒåˆ†ç±»é…ç½®
    model_config = {
        'input_size': input_size,
        'hidden_size': 256,  # ä½¿ç”¨åŸå§‹æ¡†æ¶çš„å¤§å°
        'num_layers': 4,      # ä½¿ç”¨åŸå§‹æ¡†æ¶çš„å±‚æ•°
        'num_classes': 2,     # äºŒåˆ†ç±»ï¼šæœ‰/æ— ç—…è™«å®³
        'dropout': 0.3        # ä½¿ç”¨åŸå§‹æ¡†æ¶çš„dropout
    }
    
    # åˆ›å»ºæ¨¡å‹
    model = BiLSTMModel(model_config).to(device)
    
    print(f"æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
    
    # æµ‹è¯•æ¨¡å‹è¾“å‡ºç»´åº¦
    test_input = torch.randn(1, 8, input_size).to(device)
    test_output = model(test_input)
    print(f"æµ‹è¯•è¾“å…¥å½¢çŠ¶: {test_input.shape}")
    print(f"æµ‹è¯•è¾“å‡ºå½¢çŠ¶: {test_output.shape}")
    print(f"è¾“å‡ºèŒƒå›´: {test_output.min():.4f} åˆ° {test_output.max():.4f}")
    
    # è®¡ç®—ç±»åˆ«æƒé‡ - å¤„ç†æ•°æ®ä¸å‡è¡¡
    class_counts = np.bincount(y.cpu().numpy())
    class_weights = len(y) / (len(class_counts) * class_counts)
    class_weights = torch.FloatTensor(class_weights).to(device)
    print(f"ç±»åˆ«æƒé‡: æ— ç—…è™«å®³={class_weights[0]:.2f}, æœ‰ç—…è™«å®³={class_weights[1]:.2f}")
    
    # å®šä¹‰åŠ æƒæŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    criterion = nn.CrossEntropyLoss(weight=class_weights).to(device)
    optimizer = Adam(model.parameters(), lr=0.001)
    
    # è®­ç»ƒå¾ªç¯ - åŸºäºåŸå§‹æ¡†æ¶çš„è®­ç»ƒæ–¹å¼
    print("å¼€å§‹è®­ç»ƒ...")
    model.train()
    
    epochs = 10  # å¢åŠ è®­ç»ƒè½®æ•°
    batch_size = 64  # å¢åŠ æ‰¹æ¬¡å¤§å°
    
    # è®°å½•è®­ç»ƒå†å²
    train_losses = []
    train_accuracies = []
    best_accuracy = 0
    best_model_state = None
    
    for epoch in range(epochs):
        total_loss = 0
        correct = 0
        total = 0
        
        # å°æ‰¹é‡è®­ç»ƒ
        num_batches = len(X) // batch_size
        for i in range(num_batches):
            start_idx = i * batch_size
            end_idx = start_idx + batch_size
            
            batch_x = X[start_idx:end_idx]
            batch_y = y[start_idx:end_idx]
            
            # å‰å‘ä¼ æ’­
            outputs = model(batch_x)
            loss = criterion(outputs, batch_y)
            
            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            # ç»Ÿè®¡
            total_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total += batch_y.size(0)
            correct += (predicted == batch_y).sum().item()
        
        avg_loss = total_loss / num_batches
        accuracy = 100. * correct / total
        
        train_losses.append(avg_loss)
        train_accuracies.append(accuracy)
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if accuracy > best_accuracy:
            best_accuracy = accuracy
            best_model_state = model.state_dict().copy()
            print(f"  ğŸ¯ æ–°æœ€ä½³å‡†ç¡®ç‡: {best_accuracy:.2f}%")
        
        print(f"Epoch {epoch+1}/{epochs}")
        print(f"  æŸå¤±: {avg_loss:.4f}")
        print(f"  å‡†ç¡®ç‡: {accuracy:.2f}%")
        
        # æ¯10ä¸ªepochæ˜¾ç¤ºä¸€æ¬¡è¯¦ç»†åˆ†æ
        if (epoch + 1) % 10 == 0:
            print(f"  æ ‡ç­¾åˆ†å¸ƒ: {np.bincount(batch_y.cpu().numpy())}")
    
    # è¯„ä¼°æ¨¡å‹
    print("\nè¯„ä¼°æ¨¡å‹...")
    model.eval()
    
    with torch.no_grad():
        outputs = model(X)
        _, predicted = torch.max(outputs.data, 1)
        
        # è®¡ç®—æŒ‡æ ‡
        accuracy = accuracy_score(labels, predicted.cpu().numpy())
        f1 = f1_score(labels, predicted.cpu().numpy(), average='weighted')
        precision = precision_score(labels, predicted.cpu().numpy(), average='weighted')
        recall = recall_score(labels, predicted.cpu().numpy(), average='weighted')
        
        print(f"è®­ç»ƒé›†å‡†ç¡®ç‡: {accuracy:.4f}")
        print(f"è®­ç»ƒé›†F1åˆ†æ•°: {f1:.4f}")
        print(f"è®­ç»ƒé›†ç²¾ç¡®ç‡: {precision:.4f}")
        print(f"è®­ç»ƒé›†å¬å›ç‡: {recall:.4f}")
        
        # è¯¦ç»†æŠ¥å‘Š
        print("\nåˆ†ç±»æŠ¥å‘Š:")
        print(classification_report(
            labels, 
            predicted.cpu().numpy(),
            target_names=['ä½é£é™©', 'ä¸­é£é™©', 'é«˜é£é™©']
        ))
    
    # åŠ è½½æœ€ä½³æ¨¡å‹çŠ¶æ€
    if best_model_state is not None:
        model.load_state_dict(best_model_state)
        print(f"\nâœ… åŠ è½½æœ€ä½³æ¨¡å‹çŠ¶æ€ (æœ€ä½³å‡†ç¡®ç‡: {best_accuracy:.2f}%)")
    
    # æœ€ç»ˆè¯„ä¼°
    print("\næœ€ç»ˆè¯„ä¼°...")
    model.eval()
    
    with torch.no_grad():
        outputs = model(X)
        _, predicted = torch.max(outputs.data, 1)
        
        # è®¡ç®—æŒ‡æ ‡ - é’ˆå¯¹ä¸å‡è¡¡æ•°æ®çš„è¯„ä¼°
        accuracy = accuracy_score(labels, predicted.cpu().numpy())
        f1 = f1_score(labels, predicted.cpu().numpy(), average='weighted')
        f1_binary = f1_score(labels, predicted.cpu().numpy())
        precision = precision_score(labels, predicted.cpu().numpy(), average='weighted')
        recall = recall_score(labels, predicted.cpu().numpy(), average='weighted')
        
        # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„æŒ‡æ ‡
        precision_per_class = precision_score(labels, predicted.cpu().numpy(), average=None)
        recall_per_class = recall_score(labels, predicted.cpu().numpy(), average=None)
        
        print(f"æœ€ç»ˆè®­ç»ƒé›†å‡†ç¡®ç‡: {accuracy:.4f}")
        print(f"æœ€ç»ˆè®­ç»ƒé›†F1åˆ†æ•° (weighted): {f1:.4f}")
        print(f"æœ€ç»ˆè®­ç»ƒé›†F1åˆ†æ•° (binary): {f1_binary:.4f}")
        print(f"æœ€ç»ˆè®­ç»ƒé›†ç²¾ç¡®ç‡ (weighted): {precision:.4f}")
        print(f"æœ€ç»ˆè®­ç»ƒé›†å¬å›ç‡ (weighted): {recall:.4f}")
        
        # æ¯ä¸ªç±»åˆ«çš„è¯¦ç»†æŒ‡æ ‡
        print(f"\nå„ç±»åˆ«è¯¦ç»†æŒ‡æ ‡:")
        print(f"æ— ç—…è™«å®³ - ç²¾ç¡®ç‡: {precision_per_class[0]:.4f}, å¬å›ç‡: {recall_per_class[0]:.4f}")
        print(f"æœ‰ç—…è™«å®³ - ç²¾ç¡®ç‡: {precision_per_class[1]:.4f}, å¬å›ç‡: {recall_per_class[1]:.4f}")
        
        # åˆ†æé¢„æµ‹åˆ†å¸ƒ
        print("\né¢„æµ‹åˆ†æ:")
        true_dist = np.bincount(labels)
        pred_dist = np.bincount(predicted.cpu().numpy())
        print(f"çœŸå®æ ‡ç­¾åˆ†å¸ƒ: æ— ç—…è™«å®³={true_dist[0]}, æœ‰ç—…è™«å®³={true_dist[1]}")
        print(f"é¢„æµ‹æ ‡ç­¾åˆ†å¸ƒ: æ— ç—…è™«å®³={pred_dist[0]}, æœ‰ç—…è™«å®³={pred_dist[1]}")
        
        # æ··æ·†çŸ©é˜µ
        cm = confusion_matrix(labels, predicted.cpu().numpy())
        print(f"\næ··æ·†çŸ©é˜µ:")
        print(f"                é¢„æµ‹æ— ç—…è™«å®³  é¢„æµ‹æœ‰ç—…è™«å®³")
        print(f"çœŸå®æ— ç—…è™«å®³      {cm[0,0]:6d}      {cm[0,1]:6d}")
        print(f"çœŸå®æœ‰ç—…è™«å®³      {cm[1,0]:6d}      {cm[1,1]:6d}")
        
        # è¯¦ç»†æŠ¥å‘Š
        print("\nåˆ†ç±»æŠ¥å‘Š:")
        print(classification_report(
            labels, 
            predicted.cpu().numpy(),
            target_names=['æ— ç—…è™«å®³', 'æœ‰ç—…è™«å®³'],
            zero_division=0
        ))
    
    # ä¿å­˜æ¨¡å‹ - æ·»åŠ æ—¶é—´æˆ³å’Œæ€§èƒ½ä¿¡æ¯
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    model_path = f'county_level_results/county_bilstm_model_acc_{accuracy:.2f}_{timestamp}.pth'
    os.makedirs('county_level_results', exist_ok=True)
    torch.save({
        'model_state_dict': model.state_dict(),
        'model_config': model_config,
        'input_size': input_size,
        'train_losses': train_losses,
        'train_accuracies': train_accuracies,
        'best_accuracy': best_accuracy,
        'final_accuracy': accuracy,
        'timestamp': timestamp
    }, model_path)
    
    print(f"\næ¨¡å‹å·²ä¿å­˜åˆ°: {model_path}")
    
    return model, model_path

def test_prediction():
    """æµ‹è¯•é¢„æµ‹åŠŸèƒ½ - äºŒåˆ†ç±»ç‰ˆæœ¬"""
    print("\næµ‹è¯•é¢„æµ‹åŠŸèƒ½...")
    
    # å¦‚æœæ²¡æœ‰æä¾›æ¨¡å‹è·¯å¾„ï¼ŒæŸ¥æ‰¾æœ€æ–°çš„æ¨¡å‹æ–‡ä»¶
    model_dir = 'county_level_results'
    if not os.path.exists(model_dir):
        print("æ¨¡å‹ç›®å½•ä¸å­˜åœ¨ï¼Œè¯·å…ˆè®­ç»ƒæ¨¡å‹")
        return
    
    # æŸ¥æ‰¾æœ€æ–°çš„æ¨¡å‹æ–‡ä»¶
    model_files = [f for f in os.listdir(model_dir) if f.startswith('county_bilstm_model_acc_') and f.endswith('.pth')]
    if not model_files:
        print("æ²¡æœ‰æ‰¾åˆ°è®­ç»ƒå¥½çš„æ¨¡å‹æ–‡ä»¶ï¼Œè¯·å…ˆè®­ç»ƒæ¨¡å‹")
        return
    
    # æŒ‰æ–‡ä»¶åæ’åºï¼Œè·å–æœ€æ–°çš„æ¨¡å‹
    model_files.sort()
    model_path = os.path.join(model_dir, model_files[-1])
    
    print(f"åŠ è½½æ¨¡å‹: {model_path}")
    
    checkpoint = torch.load(model_path)
    model_config = checkpoint['model_config']
    input_size = checkpoint['input_size']
    
    # é‡å»ºæ¨¡å‹
    model = BiLSTMModel(model_config).to(device)
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()
    
    # åŠ è½½éªŒè¯æ•°æ®
    sequences, labels, _ = load_and_prepare_data(
        'datas/shandong_pest_data/spatial_val_data.csv',
        8
    )
    
    # è½¬æ¢ä¸ºPyTorchå¼ é‡
    X = torch.FloatTensor(sequences).to(device)
    
    # è¿›è¡Œé¢„æµ‹
    with torch.no_grad():
        outputs = model(X)
        probabilities = torch.softmax(outputs, dim=1)
        _, predicted = torch.max(outputs.data, 1)
    
    # è®¡ç®—å‡†ç¡®ç‡å’Œè¯¦ç»†æŒ‡æ ‡
    accuracy = accuracy_score(labels, predicted.cpu().numpy())
    f1_binary = f1_score(labels, predicted.cpu().numpy())
    precision_per_class = precision_score(labels, predicted.cpu().numpy(), average=None)
    recall_per_class = recall_score(labels, predicted.cpu().numpy(), average=None)
    
    print(f"éªŒè¯é›†å‡†ç¡®ç‡: {accuracy:.4f}")
    print(f"éªŒè¯é›†F1åˆ†æ•°: {f1_binary:.4f}")
    
    # åˆ†æé¢„æµ‹åˆ†å¸ƒ
    print("\néªŒè¯é›†é¢„æµ‹åˆ†æ:")
    true_dist = np.bincount(labels)
    pred_dist = np.bincount(predicted.cpu().numpy())
    print(f"çœŸå®æ ‡ç­¾åˆ†å¸ƒ: æ— ç—…è™«å®³={true_dist[0]}, æœ‰ç—…è™«å®³={true_dist[1]}")
    print(f"é¢„æµ‹æ ‡ç­¾åˆ†å¸ƒ: æ— ç—…è™«å®³={pred_dist[0]}, æœ‰ç—…è™«å®³={pred_dist[1]}")
    
    # å„ç±»åˆ«è¯¦ç»†æŒ‡æ ‡
    print(f"\néªŒè¯é›†å„ç±»åˆ«æŒ‡æ ‡:")
    print(f"æ— ç—…è™«å®³ - ç²¾ç¡®ç‡: {precision_per_class[0]:.4f}, å¬å›ç‡: {recall_per_class[0]:.4f}")
    print(f"æœ‰ç—…è™«å®³ - ç²¾ç¡®ç‡: {precision_per_class[1]:.4f}, å¬å›ç‡: {recall_per_class[1]:.4f}")
    
    # æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(labels, predicted.cpu().numpy())
    print(f"\néªŒè¯é›†æ··æ·†çŸ©é˜µ:")
    print(f"                é¢„æµ‹æ— ç—…è™«å®³  é¢„æµ‹æœ‰ç—…è™«å®³")
    print(f"çœŸå®æ— ç—…è™«å®³      {cm[0,0]:6d}      {cm[0,1]:6d}")
    print(f"çœŸå®æœ‰ç—…è™«å®³      {cm[1,0]:6d}      {cm[1,1]:6d}")
    
    # æ˜¾ç¤ºä¸€äº›é¢„æµ‹ç»“æœ
    print("\né¢„æµ‹ç»“æœç¤ºä¾‹:")
    pest_status = ['æ— ç—…è™«å®³', 'æœ‰ç—…è™«å®³']
    
    for i in range(min(10, len(predicted))):
        true_label = pest_status[labels[i]]
        pred_label = pest_status[predicted[i].item()]
        confidence = probabilities[i][predicted[i]].item()
        
        print(f"æ ·æœ¬ {i+1}: çœŸå®={true_label}, é¢„æµ‹={pred_label}, ç½®ä¿¡åº¦={confidence:.2%}")

if __name__ == "__main__":
    print("å¼€å§‹å¿çº§BiLSTMå¿«é€Ÿè®­ç»ƒï¼ˆäºŒåˆ†ç±»ç‰ˆæœ¬ï¼‰...")
    
    # è®­ç»ƒæ¨¡å‹
    model, model_path = train_model()
    
    # æµ‹è¯•é¢„æµ‹
    test_prediction()
    
    print("\nè®­ç»ƒå®Œæˆï¼")
    print("ç”Ÿæˆçš„æ–‡ä»¶:")
    print(f"  - {model_path}: è®­ç»ƒå¥½çš„æ¨¡å‹")
    print("ä»»åŠ¡ï¼šåŸºäºå¿åŸŸæ°”è±¡æ•°æ®é¢„æµ‹ç¾å›½ç™½è›¾ç—…è™«å®³å‘ç”Ÿæƒ…å†µ")
    print("ç±»åˆ«ï¼š0=æ— ç—…è™«å®³, 1=æœ‰ç—…è™«å®³")