#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¨¡å‹æ€§èƒ½ç»¼åˆè¯„ä¼°ç³»ç»Ÿ
è¾“å‡ºè¯¦ç»†çš„æ¨¡å‹è¯„ä¼°æŒ‡æ ‡å’Œå¯è§†åŒ–ç»“æœ
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import (
    accuracy_score, f1_score, precision_score, recall_score,
    classification_report, confusion_matrix, roc_auc_score,
    roc_curve, precision_recall_curve, average_precision_score
)
import torch
import torch.nn.functional as F
import json
import os
from datetime import datetime

from enhanced_county_config import EnhancedCountyLevelConfig
from enhanced_model_trainer import EnhancedModelTrainer, EnhancedDataset

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False

class ModelPerformanceEvaluator:
    """æ¨¡å‹æ€§èƒ½è¯„ä¼°å™¨"""

    def __init__(self):
        self.config = EnhancedCountyLevelConfig()
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")

    def load_trained_model(self):
        """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
        print("=== åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹ ===")

        # åˆ›å»ºæ¨¡å‹
        from enhanced_model_trainer import EnhancedBiLSTMGCNModel
        model = EnhancedBiLSTMGCNModel(
            input_size=self.config.NUM_FEATURES,
            hidden_size=64,
            num_classes=self.config.NUM_CLASSES,
            dropout=0.3
        ).to(self.device)

        # åŠ è½½æƒé‡
        model_path = os.path.join(self.config.MODEL_SAVE_DIR, 'enhanced_bilstm_gcn_model.pth')
        if os.path.exists(model_path):
            model.load_state_dict(torch.load(model_path, map_location=self.device))
            model.eval()
            print(f"æ¨¡å‹æƒé‡åŠ è½½æˆåŠŸ: {model_path}")
        else:
            print(f"æ¨¡å‹æƒé‡æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
            return None

        # åŠ è½½æ¨¡å‹ä¿¡æ¯
        info_path = os.path.join(self.config.MODEL_SAVE_DIR, 'enhanced_model_info.json')
        if os.path.exists(info_path):
            with open(info_path, 'r', encoding='utf-8') as f:
                model_info = json.load(f)
            print(f"æ¨¡å‹ä¿¡æ¯: {model_info['model_type']}")
            print(f"è¾“å…¥ç‰¹å¾: {model_info['input_size']}")
            print(f"è®­ç»ƒæ—¥æœŸ: {model_info['training_date']}")

        self.model = model
        return model

    def load_test_data(self):
        """åŠ è½½æµ‹è¯•æ•°æ®"""
        print("\n=== åŠ è½½æµ‹è¯•æ•°æ® ===")

        # åŠ è½½å®Œæ•´æ•°æ®
        data = pd.read_csv(self.config.ENHANCED_COMPLETE_DATA_PATH)
        test_data = data[data['Year'].isin(self.config.TEST_YEARS)]

        print(f"æµ‹è¯•æ•°æ®: {len(test_data)} æ ·æœ¬")
        print(f"è¦†ç›–å¿æ•°: {test_data['County'].nunique()}")

        # åˆ›å»ºæ•°æ®é›†
        test_dataset = EnhancedDataset(test_data)
        print(f"æµ‹è¯•åºåˆ—æ ·æœ¬: {len(test_dataset)}")

        # è·å–æ•°æ®åŠ è½½å™¨
        from torch.utils.data import DataLoader
        test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

        return test_loader, test_dataset, test_data

    def generate_predictions(self, test_loader, test_dataset):
        """ç”Ÿæˆæ¨¡å‹é¢„æµ‹ç»“æœ"""
        print("\n=== ç”Ÿæˆé¢„æµ‹ç»“æœ ===")

        self.model.eval()
        all_predictions = []
        all_probabilities = []
        all_targets = []
        all_counties = []
        all_years = []

        with torch.no_grad():
            for batch in test_loader:
                sequences = batch['sequence'].to(self.device)
                targets = batch['target'].squeeze(-1).to(self.device)

                outputs = self.model(sequences)
                probabilities = F.softmax(outputs, dim=1)
                predictions = torch.argmax(outputs, dim=1)

                all_predictions.extend(predictions.cpu().numpy())
                all_probabilities.extend(probabilities.cpu().numpy())
                all_targets.extend(targets.cpu().numpy())
                all_counties.extend(batch['county'])
                all_years.extend(batch['year'].numpy())

        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        all_predictions = np.array(all_predictions)
        all_probabilities = np.array(all_probabilities)
        all_targets = np.array(all_targets)

        print(f"é¢„æµ‹å®Œæˆ: {len(all_predictions)} ä¸ªæ ·æœ¬")
        return all_predictions, all_probabilities, all_targets, all_counties, all_years

    def calculate_comprehensive_metrics(self, y_true, y_pred, y_prob):
        """è®¡ç®—ç»¼åˆè¯„ä¼°æŒ‡æ ‡"""
        print("\n=== è®¡ç®—ç»¼åˆè¯„ä¼°æŒ‡æ ‡ ===")

        metrics = {}

        # åŸºç¡€åˆ†ç±»æŒ‡æ ‡
        metrics['accuracy'] = accuracy_score(y_true, y_pred)
        metrics['precision_macro'] = precision_score(y_true, y_pred, average='macro', zero_division=0)
        metrics['precision_weighted'] = precision_score(y_true, y_pred, average='weighted', zero_division=0)
        metrics['recall_macro'] = recall_score(y_true, y_pred, average='macro', zero_division=0)
        metrics['recall_weighted'] = recall_score(y_true, y_pred, average='weighted', zero_division=0)
        metrics['f1_macro'] = f1_score(y_true, y_pred, average='macro', zero_division=0)
        metrics['f1_weighted'] = f1_score(y_true, y_pred, average='weighted', zero_division=0)

        # å„ç±»åˆ«è¯¦ç»†æŒ‡æ ‡
        precision_per_class = precision_score(y_true, y_pred, average=None, zero_division=0)
        recall_per_class = recall_score(y_true, y_pred, average=None, zero_division=0)
        f1_per_class = f1_score(y_true, y_pred, average=None, zero_division=0)

        metrics['per_class_metrics'] = {}
        for i, class_name in enumerate(self.config.CLASS_NAMES):
            if i < len(precision_per_class):
                metrics['per_class_metrics'][class_name] = {
                    'precision': float(precision_per_class[i]) if i < len(precision_per_class) else 0.0,
                    'recall': float(recall_per_class[i]) if i < len(recall_per_class) else 0.0,
                    'f1': float(f1_per_class[i]) if i < len(f1_per_class) else 0.0
                }

        # å°è¯•è®¡ç®—AUCï¼ˆå¯¹äºå¤šåˆ†ç±»ï¼‰
        try:
            if len(np.unique(y_true)) > 1:  # ç¡®ä¿æœ‰å¤šä¸ªç±»åˆ«
                metrics['auc_macro'] = roc_auc_score(y_true, y_prob, average='macro', multi_class='ovr')
                metrics['auc_weighted'] = roc_auc_score(y_true, y_prob, average='weighted', multi_class='ovr')
            else:
                metrics['auc_macro'] = 0.0
                metrics['auc_weighted'] = 0.0
        except:
            metrics['auc_macro'] = 0.0
            metrics['auc_weighted'] = 0.0

        return metrics

    def print_detailed_metrics(self, metrics):
        """æ‰“å°è¯¦ç»†æŒ‡æ ‡"""
        print("\n" + "="*80)
        print("æ¨¡å‹æ€§èƒ½è¯¦ç»†è¯„ä¼°æŠ¥å‘Š")
        print("="*80)

        print(f"\nğŸ“Š æ•´ä½“æ€§èƒ½æŒ‡æ ‡:")
        print(f"  å‡†ç¡®ç‡ (Accuracy):           {metrics['accuracy']:.4f}")
        print(f"  ç²¾ç¡®ç‡-å®å¹³å‡ (Precision):    {metrics['precision_macro']:.4f}")
        print(f"  ç²¾ç¡®ç‡-åŠ æƒ (Precision):      {metrics['precision_weighted']:.4f}")
        print(f"  å¬å›ç‡-å®å¹³å‡ (Recall):       {metrics['recall_macro']:.4f}")
        print(f"  å¬å›ç‡-åŠ æƒ (Recall):         {metrics['recall_weighted']:.4f}")
        print(f"  F1åˆ†æ•°-å®å¹³å‡ (F1-Score):     {metrics['f1_macro']:.4f}")
        print(f"  F1åˆ†æ•°-åŠ æƒ (F1-Score):       {metrics['f1_weighted']:.4f}")

        if 'auc_macro' in metrics:
            print(f"  AUC-å®å¹³å‡:                  {metrics['auc_macro']:.4f}")
            print(f"  AUC-åŠ æƒ:                    {metrics['auc_weighted']:.4f}")

        print(f"\nğŸ“ˆ å„ç±»åˆ«è¯¦ç»†æŒ‡æ ‡:")
        print(f"{'ç±»åˆ«':<8} {'ç²¾ç¡®ç‡':<8} {'å¬å›ç‡':<8} {'F1åˆ†æ•°':<8}")
        print("-" * 35)
        for class_name, class_metrics in metrics['per_class_metrics'].items():
            print(f"{class_name:<8} {class_metrics['precision']:<8.4f} {class_metrics['recall']:<8.4f} {class_metrics['f1']:<8.4f}")

    def create_comprehensive_visualizations(self, y_true, y_pred, y_prob, counties, years):
        """åˆ›å»ºç»¼åˆå¯è§†åŒ–"""
        print("\n=== åˆ›å»ºç»¼åˆå¯è§†åŒ– ===")

        os.makedirs('results/enhanced_visualizations', exist_ok=True)

        # 1. æ··æ·†çŸ©é˜µ
        self.create_enhanced_confusion_matrix(y_true, y_pred)

        # 2. ROCæ›²çº¿ï¼ˆå¤šåˆ†ç±»ï¼‰
        if len(np.unique(y_true)) > 1:
            self.create_multiclass_roc_curve(y_true, y_prob)

        # 3. ç²¾ç¡®ç‡-å¬å›ç‡æ›²çº¿
        if len(np.unique(y_true)) > 1:
            self.create_precision_recall_curve(y_true, y_prob)

        # 4. é¢„æµ‹ç½®ä¿¡åº¦åˆ†æ
        self.create_prediction_confidence_analysis(y_true, y_pred, y_prob)

        # 5. å¿çº§é¢„æµ‹ç»“æœåˆ†æ
        self.create_county_prediction_analysis(counties, years, y_true, y_pred)

        # 6. ç±»åˆ«åˆ†å¸ƒå¯¹æ¯”
        self.create_class_distribution_comparison(y_true, y_pred)

        print("å¯è§†åŒ–å›¾è¡¨ä¿å­˜å®Œæˆ")

    def create_enhanced_confusion_matrix(self, y_true, y_pred):
        """åˆ›å»ºå¢å¼ºç‰ˆæ··æ·†çŸ©é˜µ"""
        cm = confusion_matrix(y_true, y_pred)

        plt.figure(figsize=(10, 8))

        # è®¡ç®—å½’ä¸€åŒ–æ··æ·†çŸ©é˜µ
        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]

        # åˆ›å»ºå­å›¾
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))

        # ç»å¯¹æ•°é‡æ··æ·†çŸ©é˜µ
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                   xticklabels=self.config.CLASS_NAMES[:len(cm)],
                   yticklabels=self.config.CLASS_NAMES[:len(cm)],
                   ax=ax1, cbar_kws={'label': 'æ ·æœ¬æ•°'})
        ax1.set_title('æ··æ·†çŸ©é˜µ (ç»å¯¹æ•°é‡)', fontsize=14, fontweight='bold')
        ax1.set_xlabel('é¢„æµ‹ç±»åˆ«')
        ax1.set_ylabel('çœŸå®ç±»åˆ«')

        # å½’ä¸€åŒ–æ··æ·†çŸ©é˜µ
        sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues',
                   xticklabels=self.config.CLASS_NAMES[:len(cm_normalized)],
                   yticklabels=self.config.CLASS_NAMES[:len(cm_normalized)],
                   ax=ax2, cbar_kws={'label': 'æ¯”ä¾‹'})
        ax2.set_title('æ··æ·†çŸ©é˜µ (å½’ä¸€åŒ–)', fontsize=14, fontweight='bold')
        ax2.set_xlabel('é¢„æµ‹ç±»åˆ«')
        ax2.set_ylabel('çœŸå®ç±»åˆ«')

        plt.tight_layout()
        plt.savefig('results/enhanced_visualizations/enhanced_confusion_matrix.png', dpi=300, bbox_inches='tight')
        plt.close()

    def create_multiclass_roc_curve(self, y_true, y_prob):
        """åˆ›å»ºå¤šåˆ†ç±»ROCæ›²çº¿"""
        from sklearn.preprocessing import label_binarize
        from sklearn.metrics import roc_curve, auc

        # äºŒå€¼åŒ–æ ‡ç­¾
        y_true_bin = label_binarize(y_true, classes=range(self.config.NUM_CLASSES))

        plt.figure(figsize=(12, 8))

        # ä¸ºæ¯ä¸ªç±»åˆ«ç»˜åˆ¶ROCæ›²çº¿
        for i in range(self.config.NUM_CLASSES):
            if i < y_prob.shape[1] and i < y_true_bin.shape[1]:
                fpr, tpr, _ = roc_curve(y_true_bin[:, i], y_prob[:, i])
                roc_auc = auc(fpr, tpr)
                plt.plot(fpr, tpr, linewidth=2,
                        label=f'{self.config.CLASS_NAMES[i]} (AUC = {roc_auc:.3f})')

        plt.plot([0, 1], [0, 1], 'k--', linewidth=1, label='éšæœºåˆ†ç±»å™¨')
        plt.xlim([0.0, 1.0])
        plt.ylim([0.0, 1.05])
        plt.xlabel('å‡æ­£ç‡ (FPR)', fontsize=12)
        plt.ylabel('çœŸæ­£ç‡ (TPR)', fontsize=12)
        plt.title('å¤šåˆ†ç±»ROCæ›²çº¿', fontsize=14, fontweight='bold')
        plt.legend(loc="lower right", fontsize=10)
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.savefig('results/enhanced_visualizations/multiclass_roc_curve.png', dpi=300, bbox_inches='tight')
        plt.close()

    def create_precision_recall_curve(self, y_true, y_prob):
        """åˆ›å»ºç²¾ç¡®ç‡-å¬å›ç‡æ›²çº¿"""
        from sklearn.preprocessing import label_binarize

        y_true_bin = label_binarize(y_true, classes=range(self.config.NUM_CLASSES))

        plt.figure(figsize=(12, 8))

        for i in range(self.config.NUM_CLASSES):
            if i < y_prob.shape[1] and i < y_true_bin.shape[1]:
                precision, recall, _ = precision_recall_curve(y_true_bin[:, i], y_prob[:, i])
                avg_precision = average_precision_score(y_true_bin[:, i], y_prob[:, i])
                plt.plot(recall, precision, linewidth=2,
                        label=f'{self.config.CLASS_NAMES[i]} (AP = {avg_precision:.3f})')

        plt.xlabel('å¬å›ç‡ (Recall)', fontsize=12)
        plt.ylabel('ç²¾ç¡®ç‡ (Precision)', fontsize=12)
        plt.title('ç²¾ç¡®ç‡-å¬å›ç‡æ›²çº¿', fontsize=14, fontweight='bold')
        plt.legend(loc="lower left", fontsize=10)
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.savefig('results/enhanced_visualizations/precision_recall_curve.png', dpi=300, bbox_inches='tight')
        plt.close()

    def create_prediction_confidence_analysis(self, y_true, y_pred, y_prob):
        """åˆ›å»ºé¢„æµ‹ç½®ä¿¡åº¦åˆ†æ"""
        plt.figure(figsize=(15, 10))

        # 1. é¢„æµ‹ç½®ä¿¡åº¦åˆ†å¸ƒ
        plt.subplot(2, 3, 1)
        max_probs = np.max(y_prob, axis=1)
        plt.hist(max_probs, bins=20, alpha=0.7, color='skyblue', edgecolor='black')
        plt.xlabel('æœ€å¤§é¢„æµ‹æ¦‚ç‡')
        plt.ylabel('é¢‘æ¬¡')
        plt.title('é¢„æµ‹ç½®ä¿¡åº¦åˆ†å¸ƒ')
        plt.grid(True, alpha=0.3)

        # 2. æ­£ç¡®é¢„æµ‹ vs é”™è¯¯é¢„æµ‹çš„ç½®ä¿¡åº¦
        plt.subplot(2, 3, 2)
        correct_mask = (y_true == y_pred)
        correct_probs = max_probs[correct_mask]
        incorrect_probs = max_probs[~correct_mask]

        if len(incorrect_probs) > 0:
            plt.hist([correct_probs, incorrect_probs], bins=15, alpha=0.7,
                    label=['æ­£ç¡®é¢„æµ‹', 'é”™è¯¯é¢„æµ‹'], color=['green', 'red'])
            plt.legend()
        else:
            plt.hist(correct_probs, bins=15, alpha=0.7, color='green', label='æ­£ç¡®é¢„æµ‹')
            plt.legend()
        plt.xlabel('é¢„æµ‹ç½®ä¿¡åº¦')
        plt.ylabel('é¢‘æ¬¡')
        plt.title('é¢„æµ‹ç»“æœç½®ä¿¡åº¦å¯¹æ¯”')
        plt.grid(True, alpha=0.3)

        # 3. å„ç±»åˆ«é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒ
        plt.subplot(2, 3, 3)
        for i in range(min(self.config.NUM_CLASSES, len(self.config.CLASS_NAMES))):
            if i < y_prob.shape[1]:
                class_probs = y_prob[y_true == i, i] if np.any(y_true == i) else []
                if len(class_probs) > 0:
                    plt.hist(class_probs, bins=10, alpha=0.5,
                            label=f'{self.config.CLASS_NAMES[i]}')
        plt.xlabel('é¢„æµ‹æ¦‚ç‡')
        plt.ylabel('é¢‘æ¬¡')
        plt.title('å„ç±»åˆ«é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒ')
        plt.legend()
        plt.grid(True, alpha=0.3)

        # 4. é¢„æµ‹æ¦‚ç‡çƒ­åŠ›å›¾
        plt.subplot(2, 3, 4)
        if len(y_prob) > 0:
            # å–å‰20ä¸ªæ ·æœ¬çš„çƒ­åŠ›å›¾
            sample_probs = y_prob[:min(20, len(y_prob))]
            im = plt.imshow(sample_probs.T, cmap='YlOrRd', aspect='auto')
            plt.colorbar(im, label='é¢„æµ‹æ¦‚ç‡')
            plt.xlabel('æ ·æœ¬åºå·')
            plt.ylabel('é¢„æµ‹ç±»åˆ«')
            plt.title('é¢„æµ‹æ¦‚ç‡çƒ­åŠ›å›¾ (å‰20ä¸ªæ ·æœ¬)')
            plt.yticks(range(self.config.NUM_CLASSES), self.config.CLASS_NAMES)

        # 5. ç½®ä¿¡åº¦vså‡†ç¡®æ€§
        plt.subplot(2, 3, 5)
        confidence_bins = np.linspace(0.5, 1.0, 10)
        accuracy_by_confidence = []

        for i in range(len(confidence_bins) - 1):
            mask = (max_probs >= confidence_bins[i]) & (max_probs < confidence_bins[i+1])
            if np.sum(mask) > 0:
                accuracy = np.mean(y_true[mask] == y_pred[mask])
                accuracy_by_confidence.append(accuracy)
            else:
                accuracy_by_confidence.append(0)

        bin_centers = (confidence_bins[:-1] + confidence_bins[1:]) / 2
        plt.plot(bin_centers, accuracy_by_confidence, 'o-', linewidth=2, markersize=6)
        plt.xlabel('ç½®ä¿¡åº¦åŒºé—´')
        plt.ylabel('å‡†ç¡®ç‡')
        plt.title('ç½®ä¿¡åº¦ vs å‡†ç¡®ç‡å…³ç³»')
        plt.grid(True, alpha=0.3)

        # 6. é¢„æµ‹ç±»åˆ«æ¦‚ç‡åˆ†å¸ƒ
        plt.subplot(2, 3, 6)
        class_predictions = np.argmax(y_prob, axis=1)
        class_counts = np.bincount(class_predictions, minlength=self.config.NUM_CLASSES)
        colors = self.config.CLASS_COLORS[:len(class_counts)]

        bars = plt.bar(range(len(class_counts)), class_counts, color=colors, alpha=0.7)
        plt.xlabel('é¢„æµ‹ç±»åˆ«')
        plt.ylabel('æ ·æœ¬æ•°')
        plt.title('é¢„æµ‹ç±»åˆ«åˆ†å¸ƒ')
        plt.xticks(range(len(self.config.CLASS_NAMES)), self.config.CLASS_NAMES)
        plt.grid(True, alpha=0.3)

        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, count in zip(bars, class_counts):
            plt.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.1,
                    f'{count}', ha='center', va='bottom')

        plt.tight_layout()
        plt.savefig('results/enhanced_visualizations/prediction_confidence_analysis.png',
                   dpi=300, bbox_inches='tight')
        plt.close()

    def create_county_prediction_analysis(self, counties, years, y_true, y_pred):
        """åˆ›å»ºå¿çº§é¢„æµ‹ç»“æœåˆ†æ"""
        plt.figure(figsize=(15, 10))

        # åˆ›å»ºç»“æœDataFrame
        results_df = pd.DataFrame({
            'County': counties,
            'Year': years,
            'Actual': y_true,
            'Predicted': y_pred,
            'Correct': (y_true == y_pred).astype(int)
        })

        # 1. å„å¿é¢„æµ‹å‡†ç¡®æ€§
        plt.subplot(2, 3, 1)
        county_accuracy = results_df.groupby('County')['Correct'].mean()
        plt.hist(county_accuracy, bins=10, alpha=0.7, color='lightgreen', edgecolor='black')
        plt.xlabel('å¿çº§é¢„æµ‹å‡†ç¡®ç‡')
        plt.ylabel('å¿æ•°')
        plt.title('å„å¿é¢„æµ‹å‡†ç¡®ç‡åˆ†å¸ƒ')
        plt.grid(True, alpha=0.3)

        # 2. å¹´åº¦é¢„æµ‹å‡†ç¡®æ€§
        plt.subplot(2, 3, 2)
        year_accuracy = results_df.groupby('Year')['Correct'].mean()
        plt.bar(year_accuracy.index, year_accuracy.values, color='lightblue', alpha=0.7)
        plt.xlabel('å¹´ä»½')
        plt.ylabel('é¢„æµ‹å‡†ç¡®ç‡')
        plt.title('å¹´åº¦é¢„æµ‹å‡†ç¡®ç‡')
        plt.xticks(year_accuracy.index)
        plt.grid(True, alpha=0.3)

        # 3. å®é™…vsé¢„æµ‹ç±»åˆ«åˆ†å¸ƒå¯¹æ¯”
        plt.subplot(2, 3, 3)
        actual_dist = results_df['Actual'].value_counts().sort_index()
        pred_dist = results_df['Predicted'].value_counts().sort_index()

        x = np.arange(len(actual_dist))
        width = 0.35

        plt.bar(x - width/2, actual_dist.values, width, label='å®é™…', alpha=0.7, color='blue')
        plt.bar(x + width/2, pred_dist.values, width, label='é¢„æµ‹', alpha=0.7, color='red')

        plt.xlabel('å‘ç—…ç­‰çº§')
        plt.ylabel('æ ·æœ¬æ•°')
        plt.title('å®é™…vsé¢„æµ‹åˆ†å¸ƒå¯¹æ¯”')
        plt.xticks(x, [self.config.CLASS_NAMES[i] for i in actual_dist.index])
        plt.legend()
        plt.grid(True, alpha=0.3)

        # 4. é”™è¯¯é¢„æµ‹åˆ†æ
        plt.subplot(2, 3, 4)
        incorrect_results = results_df[results_df['Correct'] == 0]
        if len(incorrect_results) > 0:
            error_matrix = pd.crosstab(incorrect_results['Actual'],
                                     incorrect_results['Predicted'],
                                     normalize='index')
            sns.heatmap(error_matrix, annot=True, fmt='.2f', cmap='Reds',
                       xticklabels=[self.config.CLASS_NAMES[i] for i in error_matrix.columns],
                       yticklabels=[self.config.CLASS_NAMES[i] for i in error_matrix.index])
            plt.title('é”™è¯¯é¢„æµ‹æ¨¡å¼')
            plt.xlabel('é¢„æµ‹ç±»åˆ«')
            plt.ylabel('å®é™…ç±»åˆ«')
        else:
            plt.text(0.5, 0.5, 'æ— é”™è¯¯é¢„æµ‹', ha='center', va='center',
                    transform=plt.gca().transAxes, fontsize=14)
            plt.title('é”™è¯¯é¢„æµ‹æ¨¡å¼')

        # 5. å„å¿æ ·æœ¬æ•°é‡åˆ†å¸ƒ
        plt.subplot(2, 3, 5)
        county_samples = results_df.groupby('County').size()
        plt.hist(county_samples, bins=15, alpha=0.7, color='orange', edgecolor='black')
        plt.xlabel('æ¯å¿æ ·æœ¬æ•°')
        plt.ylabel('å¿æ•°')
        plt.title('å„å¿æ ·æœ¬æ•°é‡åˆ†å¸ƒ')
        plt.grid(True, alpha=0.3)

        # 6. é¢„æµ‹ç»“æœæ—¶é—´åºåˆ—
        plt.subplot(2, 3, 6)
        time_accuracy = results_df.groupby(['Year', 'County'])['Correct'].mean().groupby('Year').mean()
        plt.plot(time_accuracy.index, time_accuracy.values, 'o-', linewidth=2, markersize=8)
        plt.xlabel('å¹´ä»½')
        plt.ylabel('å¹³å‡å‡†ç¡®ç‡')
        plt.title('å¹´åº¦å¹³å‡é¢„æµ‹å‡†ç¡®ç‡è¶‹åŠ¿')
        plt.grid(True, alpha=0.3)

        plt.tight_layout()
        plt.savefig('results/enhanced_visualizations/county_prediction_analysis.png',
                   dpi=300, bbox_inches='tight')
        plt.close()

    def create_class_distribution_comparison(self, y_true, y_pred):
        """åˆ›å»ºç±»åˆ«åˆ†å¸ƒå¯¹æ¯”"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))

        # 1. å®é™…vsé¢„æµ‹åˆ†å¸ƒ - æŸ±çŠ¶å›¾
        ax1 = axes[0, 0]
        actual_counts = np.bincount(y_true, minlength=self.config.NUM_CLASSES)
        pred_counts = np.bincount(y_pred, minlength=self.config.NUM_CLASSES)

        x = np.arange(self.config.NUM_CLASSES)
        width = 0.35

        bars1 = ax1.bar(x - width/2, actual_counts, width, label='å®é™…', alpha=0.7, color='blue')
        bars2 = ax1.bar(x + width/2, pred_counts, width, label='é¢„æµ‹', alpha=0.7, color='red')

        ax1.set_xlabel('å‘ç—…ç­‰çº§')
        ax1.set_ylabel('æ ·æœ¬æ•°')
        ax1.set_title('å®é™…vsé¢„æµ‹æ ·æœ¬åˆ†å¸ƒ')
        ax1.set_xticks(x)
        ax1.set_xticklabels(self.config.CLASS_NAMES)
        ax1.legend()
        ax1.grid(True, alpha=0.3)

        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bars in [bars1, bars2]:
            for bar in bars:
                height = bar.get_height()
                ax1.text(bar.get_x() + bar.get_width()/2., height + 0.1,
                        f'{int(height)}', ha='center', va='bottom')

        # 2. åˆ†å¸ƒæ¯”ä¾‹å¯¹æ¯” - é¥¼å›¾
        ax2 = axes[0, 1]
        actual_labels = [f'{self.config.CLASS_NAMES[i]}\n({actual_counts[i]})'
                        for i in range(self.config.NUM_CLASSES) if actual_counts[i] > 0]
        actual_sizes = [actual_counts[i] for i in range(self.config.NUM_CLASSES) if actual_counts[i] > 0]

        ax2.pie(actual_sizes, labels=actual_labels, autopct='%1.1f%%', startangle=90)
        ax2.set_title('å®é™…åˆ†å¸ƒæ¯”ä¾‹')

        # 3. é¢„æµ‹å‡†ç¡®æ€§çŸ©é˜µ
        ax3 = axes[1, 0]
        cm = confusion_matrix(y_true, y_pred)
        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]

        sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Greens',
                   xticklabels=self.config.CLASS_NAMES[:len(cm_normalized)],
                   yticklabels=self.config.CLASS_NAMES[:len(cm_normalized)],
                   ax=ax3, cbar_kws={'label': 'å‡†ç¡®ç‡'})
        ax3.set_title('å„ç±»åˆ«é¢„æµ‹å‡†ç¡®æ€§')
        ax3.set_xlabel('é¢„æµ‹ç±»åˆ«')
        ax3.set_ylabel('å®é™…ç±»åˆ«')

        # 4. æ€§èƒ½æŒ‡æ ‡é›·è¾¾å›¾
        ax4 = axes[1, 1]
        categories = ['ç²¾ç¡®ç‡', 'å¬å›ç‡', 'F1åˆ†æ•°']

        # è®¡ç®—å„ç±»åˆ«çš„æŒ‡æ ‡
        precision_per_class = precision_score(y_true, y_pred, average=None, zero_division=0)
        recall_per_class = recall_score(y_true, y_pred, average=None, zero_division=0)
        f1_per_class = f1_score(y_true, y_pred, average=None, zero_division=0)

        # åªæ˜¾ç¤ºæœ‰æ ·æœ¬çš„ç±»åˆ«
        valid_classes = [i for i in range(self.config.NUM_CLASSES)
                        if i < len(precision_per_class) and actual_counts[i] > 0]

        if len(valid_classes) > 0:
            angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()
            angles += angles[:1]  # é—­åˆ

            valid_class_names = [self.config.CLASS_NAMES[i] for i in valid_classes]
            colors = plt.cm.Set3(np.linspace(0, 1, len(valid_classes)))

            for i, class_idx in enumerate(valid_classes):
                values = [
                    precision_per_class[class_idx],
                    recall_per_class[class_idx],
                    f1_per_class[class_idx]
                ]
                values += values[:1]  # é—­åˆ

                ax4.plot(angles, values, 'o-', linewidth=2, label=valid_class_names[i], color=colors[i])
                ax4.fill(angles, values, alpha=0.1, color=colors[i])

            ax4.set_xticks(angles[:-1])
            ax4.set_xticklabels(categories)
            ax4.set_ylim(0, 1)
            ax4.set_title('å„ç±»åˆ«æ€§èƒ½é›·è¾¾å›¾')
            ax4.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))
            ax4.grid(True)
        else:
            ax4.text(0.5, 0.5, 'æ•°æ®ä¸è¶³', ha='center', va='center',
                    transform=ax4.transAxes, fontsize=14)
            ax4.set_title('å„ç±»åˆ«æ€§èƒ½é›·è¾¾å›¾')

        plt.tight_layout()
        plt.savefig('results/enhanced_visualizations/class_distribution_comparison.png',
                   dpi=300, bbox_inches='tight')
        plt.close()

    def generate_comprehensive_report(self, metrics, test_data):
        """ç”Ÿæˆç»¼åˆè¯„ä¼°æŠ¥å‘Š"""
        print("\n=== ç”Ÿæˆç»¼åˆè¯„ä¼°æŠ¥å‘Š ===")

        report = {
            'evaluation_date': datetime.now().isoformat(),
            'model_info': {
                'type': 'EnhancedBiLSTMGCNModel',
                'input_features': self.config.NUM_FEATURES,
                'num_classes': self.config.NUM_CLASSES,
                'feature_categories': self.config.get_feature_categories()
            },
            'dataset_info': {
                'test_samples': len(test_data),
                'test_counties': test_data['County'].nunique(),
                'test_years': self.config.TEST_YEARS,
                'class_distribution': {
                    str(level): int(test_data[test_data['Severity_Level'] == level].shape[0])
                    for level in range(self.config.NUM_CLASSES)
                }
            },
            'performance_metrics': {
                'overall': {
                    'accuracy': float(metrics['accuracy']),
                    'precision_macro': float(metrics['precision_macro']),
                    'precision_weighted': float(metrics['precision_weighted']),
                    'recall_macro': float(metrics['recall_macro']),
                    'recall_weighted': float(metrics['recall_weighted']),
                    'f1_macro': float(metrics['f1_macro']),
                    'f1_weighted': float(metrics['f1_weighted'])
                },
                'per_class': metrics['per_class_metrics']
            },
            'data_enhancement_impact': {
                'total_counties_covered': 135,
                'healthy_counties_added': 25,
                'remote_sensing_features': len([f for f in self.config.ALL_FEATURES
                                              if any(x in f for x in ['NDVI', 'EVI', 'LST', 'TRMM', 'Soil'])]),
                'geographical_features': len([f for f in self.config.ALL_FEATURES
                                            if any(x in f for x in ['Coastal', 'Forest', 'Influence'])])
            },
            'visualization_files': [
                'enhanced_confusion_matrix.png',
                'multiclass_roc_curve.png',
                'precision_recall_curve.png',
                'prediction_confidence_analysis.png',
                'county_prediction_analysis.png',
                'class_distribution_comparison.png'
            ]
        }

        # ä¿å­˜æŠ¥å‘Š
        report_path = os.path.join(self.config.RESULTS_DIR, 'comprehensive_performance_report.json')
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)

        print(f"ç»¼åˆè¯„ä¼°æŠ¥å‘Šä¿å­˜åˆ°: {report_path}")
        return report

def main():
    """ä¸»å‡½æ•°"""
    print("=== æ¨¡å‹æ€§èƒ½ç»¼åˆè¯„ä¼°ç³»ç»Ÿ ===")
    print("å¼€å§‹æ—¶é—´:", datetime.now().strftime('%Y-%m-%d %H:%M:%S'))

    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = ModelPerformanceEvaluator()

    # åŠ è½½æ¨¡å‹å’Œæ•°æ®
    model = evaluator.load_trained_model()
    if model is None:
        print("æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œè¯·å…ˆè®­ç»ƒæ¨¡å‹")
        return

    test_loader, test_dataset, test_data = evaluator.load_test_data()

    # ç”Ÿæˆé¢„æµ‹
    predictions, probabilities, targets, counties, years = evaluator.generate_predictions(test_loader, test_dataset)

    # è®¡ç®—æŒ‡æ ‡
    metrics = evaluator.calculate_comprehensive_metrics(targets, predictions, probabilities)

    # æ‰“å°è¯¦ç»†æŒ‡æ ‡
    evaluator.print_detailed_metrics(metrics)

    # åˆ›å»ºå¯è§†åŒ–
    evaluator.create_comprehensive_visualizations(targets, predictions, probabilities, counties, years)

    # ç”ŸæˆæŠ¥å‘Š
    report = evaluator.generate_comprehensive_report(metrics, test_data)

    print(f"\n=== è¯„ä¼°å®Œæˆ ===")
    print(f"æ‰€æœ‰è¯„ä¼°ç»“æœå’Œå¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜åˆ° results/enhanced_visualizations/ ç›®å½•")
    print(f"ç»¼åˆæŠ¥å‘Šä¿å­˜åˆ°: results/enhanced_predictions/comprehensive_performance_report.json")
    print(f"ç»“æŸæ—¶é—´:", datetime.now().strftime('%Y-%m-%d %H:%M:%S'))

    return evaluator, report

if __name__ == "__main__":
    evaluator, report = main()